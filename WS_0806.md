# 4대로 분산



![hadoopws0806](https://user-images.githubusercontent.com/50862497/62586648-6b6a3200-b8f9-11e9-9039-726f8a8c14e4.png)





(HADOOPSERVER1 사용)

##### Master

```
[~]# hostnamectl set-hostname t1m
[~]# vi /etc/sysconfig/network-scripts/ifcfg-ens33 
HWADDR="00:0C:29:26:7C:E6"
TYPE="Ethernet"
BOOTPROTO=none
IPADDR=70.12.114.211
NETMASK=255.255.255.0
GATEWAY=70.12.114.1
DNS1=168.126.63.1
[~]# systemctl restart network
[~]# vi /etc/hosts
70.12.114.211   t1m
70.12.114.199   t1s2
70.12.114.209   t1s3
70.12.114.223   t1s4

[root@t1m ~]# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
[.ssh]# cat id_dsa.pub > authorized_keys
[root@t1m .ssh]# scp authorized_keys root@t1s2:~/.ssh/authorized_keys
[root@t1m .ssh]# scp authorized_keys root@t1s3:~/.ssh/authorized_keys
[root@t1m .ssh]# scp authorized_keys root@t1s4:~/.ssh/authorized_keys

[file]# tar xvf hadoop-1.2.1.tar.gz
[file]# cp -r hadoop-1.2.1 /etc

/etc/hadoop-1.2.1/conf
[conf]# vi hadoop-env.sh 
9  export JAVA_HOME=/etc/jdk1.8
10 export HADOOP_HOME_WARN_SUPPRESS="TRUE"

[root@t1m conf]# vi masters
t1s2
[root@t1m conf]# vi slaves
t1s3
t1s4

[root@t1m conf]# vi core-site.xml
<configuration>
        <property>
                <name>fs.default.name</name>
                <value>hdfs://70.12.114.211:9000</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/etc/hadoop-1.2.1/tmp</value>
        </property>
</configuration>

[root@t1m conf]# vi hdfs-site.xml 
<configuration>
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>
        <property>
                <name>dfs.http.address</name>
                <value>70.12.114.211:50070</value>
        </property>
        <property>
                <name>dfs.secondary.http.address</name>
                <value>70.12.114.199:50090</value>
        </property>
        <property>
                <name>dfs.name.dir</name>
                <value>/etc/hadoop-1.2.1/name</value>
        </property>
        <property>
                <name>dfs.data.dir</name>
                <value>/etc/hadoop-1.2.1/data</value>
        </property>
</configuration>

[root@t1m conf]# vi mapred-site.xml 
<configuration>
  <property>
    <name>mapred.job.tracker</name>
    <value>70.12.114.211:9001</value>
  </property>
</configuration>

# vi /etc/bashrc
. /etc/hadoop-1.2.1/conf/hadoop-env.sh
# systemctl stop firewalld
# systemctl disable firewalld
# vi /etc/profile
JAVA_HOME=/etc/jdk1.8
CLASSPATH=$JAVA_HOME/lib
TOMCAT_HOME=/root/installfiles/tomcat
HADOOP_HOME=/etc/hadoop-1.2.1
export JAVA_HOME CLASSPATH TOMCAT_HOME HADOOP_HOME
PATH=.:$JAVA_HOME/bin:$TOMCAT_HOME/bin:$HADOOP_HOME/bin:$PATH
export PATH USER LOGNAME MAIL HOSTNAME HISTSIZE HISTCONTROL


[root@t1m etc]# tar cvfz hadoop.tar.gz hadoop-1.2.1/
[root@t1m etc]# ssh root@t1s2 "cd /etc;tar xvfz hadoop.tar.gz;rm -rf hadoop.tar.gz"
[root@t1m etc]# ssh root@t1s3 "cd /etc;tar xvfz hadoop.tar.gz;rm -rf hadoop.tar.gz"
[root@t1m etc]# ssh root@t1s4 "cd /etc;tar xvfz hadoop.tar.gz;rm -rf hadoop.tar.gz"

reboot

# hadoop namenode -format
# start-all.sh  -> 모든
# jps
Jps
JobTracker
NameNode

```



##### Slaves - secondary namenode

```
# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
# jps
JPS
SECONDARY NAMENODE
```

##### Slaves 

```
# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
# jps
JPS
TrackTracker
DataNode
```

